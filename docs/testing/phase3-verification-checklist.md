# Phase 3 動作確認チェックリスト

## 実装内容

Phase 3では以下を実装しました：

1. **ストリーミング統一インターフェース**
   - `lib/localModel/chatHelper.ts` - 統一ヘルパー関数
   - `chatWithProvider` - ストリーミングと非ストリーミングを統一的に扱う
   - `createStreamingOptions` - ストリーミングオプション作成ヘルパー

2. **useAIChat.tsのストリーミング対応**
   - `sendMessage`関数にストリーミングオプション引数を追加
   - ローカルモデルでストリーミングを使用

3. **useAIAssistant.tsのストリーミング対応**
   - ストリーミング時にリアルタイムでメッセージを更新
   - チャンクを受け取るたびにUIを更新

---

## 事前準備

- [ ] Ollamaまたはllama-serverが起動している
- [ ] ローカルモデルが利用可能
- [ ] 開発サーバーが起動できる状態

---

## 1. コンパイル・型チェック

### 確認方法

```bash
# TypeScriptの型チェック
npx tsc --noEmit

# 開発サーバーを起動
npm run dev
```

### 期待される結果

- [ ] コンパイルエラーがない
- [ ] 型エラーがない
- [ ] 開発サーバーが正常に起動する

---

## 2. ストリーミング動作確認

### 2.1 ローカルモデルでのストリーミング確認

1. AIアシスタントパネルを開く
2. ローカルモデルを選択（例: `qwen2.5:7b`）
3. メッセージを送信（例: 「こんにちは。自己紹介をしてください。」）

### 期待される結果

- [ ] **初トークンが0.2秒程度で表示される**（体感速度の劇的改善）
- [ ] テキストがリアルタイムで追加されていく（ストリーミング表示）
- [ ] 「考え中...」の表示時間が短い
- [ ] エラーメッセージが表示されない

### 2.2 ストリーミングの滑らかさ確認

1. 長い回答が期待される質問を送信（例: 「1000文字程度の説明をしてください」）
2. ストリーミング表示を観察

### 期待される結果

- [ ] テキストが滑らかに追加されていく
- [ ] 途中で止まったり、一気に表示されたりしない
- [ ] チャンクごとにUIが更新される

### 2.3 非ローカルモデルでの動作確認

1. GPTモデルを選択（例: `gpt-4o-mini`）
2. メッセージを送信

### 期待される結果

- [ ] 非ストリーミングで動作する（従来通り）
- [ ] 全生成後に表示される
- [ ] エラーが発生しない

---

## 3. パフォーマンス確認

### 3.1 TTFT（初トークン時間）の確認

ブラウザの開発者ツールのコンソールで以下を確認：

```javascript
// タイミング情報がログに出力される
// timings: { ttft: 200, totalTime: 5000, tokensPerSecond: 15 }
```

### 期待される結果

- [ ] TTFTが500ms以下（理想的には200ms以下）
- [ ] タイミング情報が正しく記録される

### 3.2 生成速度の確認

### 期待される結果

- [ ] 生成速度（tok/s）が記録される
- [ ] 総時間が記録される

---

## 4. エラーケースの確認

### 4.1 ストリーミング中断

1. ストリーミング中にページをリロード
2. ストリーミング中に別のメッセージを送信

### 期待される結果

- [ ] エラーが適切に処理される
- [ ] アプリケーションがクラッシュしない

### 4.2 ネットワークエラー

1. Ollama/llama-serverを停止
2. メッセージを送信

### 期待される結果

- [ ] エラーメッセージが適切に表示される
- [ ] ストリーミングが適切に中断される

---

## 5. UI/UX確認

### 5.1 ストリーミング表示の確認

- [ ] テキストが自然に流れるように表示される
- [ ] スクロール位置が適切に調整される
- [ ] ローディングインジケーターが適切に表示/非表示される

### 5.2 メッセージ更新の確認

- [ ] メッセージが正しく更新される
- [ ] メッセージIDが正しく管理される
- [ ] タイムスタンプが正しく記録される

---

## 6. コードの動作確認ポイント

### 6.1 chatHelper.tsの確認

- [ ] `chatWithProvider`が正しく動作する
- [ ] ストリーミングと非ストリーミングが正しく切り替わる
- [ ] `createStreamingOptions`が正しく動作する

### 6.2 useAIChat.tsの確認

- [ ] `streamingOptions`引数が正しく渡される
- [ ] 再問い合わせでもストリーミングが使用される

### 6.3 useAIAssistant.tsの確認

- [ ] ストリーミング時にメッセージがリアルタイムで更新される
- [ ] `accumulatedText`が正しく管理される

---

## 7. ログ確認

### 確認すべきログ

ブラウザのコンソールで以下を確認：

- [ ] ストリーミング開始時のログ
- [ ] チャンク受信時のログ（デバッグモード時）
- [ ] ストリーミング終了時のログ
- [ ] エラーログが出力されない

---

## 8. パフォーマンス改善の確認

### 体感速度の改善

### 期待される結果

- [ ] **初表示までの時間が大幅に短縮**（80秒 → 0.2秒程度）
- [ ] ユーザーが回答を見始める時間が劇的に改善
- [ ] 総合的な体感速度が向上

---

## 問題が発生した場合

### よくある問題と対処法

1. **ストリーミングが動作しない**
   - プロバイダーがストリーミングをサポートしているか確認
   - `supportsStreaming`が`true`か確認
   - `chatStreaming`メソッドが実装されているか確認

2. **テキストが一気に表示される**
   - ストリーミングオプションが正しく渡されているか確認
   - `onToken`コールバックが正しく呼ばれているか確認

3. **メッセージが更新されない**
   - `setMessages`が正しく呼ばれているか確認
   - メッセージIDが正しいか確認

4. **エラーが発生する**
   - コンソールでエラーログを確認
   - ネットワークタブでリクエストを確認

---

## 確認完了後の次のステップ

Phase 3の動作確認が完了したら：

1. [ ] すべてのチェック項目が完了
2. [ ] 問題がないことを確認
3. [ ] パフォーマンス改善を実感
4. [ ] Phase 4（設定画面）に進む準備が整った

---

## テスト結果記録

### テスト実施日
- 日付: ___________
- 実施者: ___________

### テスト結果
- [ ] すべて成功
- [ ] 一部問題あり（詳細を以下に記録）

### パフォーマンス改善の実感
- [ ] 体感速度が劇的に改善した
- [ ] 初表示までの時間が大幅に短縮された
- [ ] ストリーミング表示が滑らかに動作した

### 問題詳細
（問題が発生した場合、ここに詳細を記録）

---

## 参考

- 設計書: `docs/architecture/llama-cpp-integration-design.md`
- パフォーマンス見積もり: `docs/architecture/llama-cpp-performance-estimate.md`
- 実装ファイル:
  - `lib/localModel/chatHelper.ts`
  - `components/AIAssistantPanel/hooks/useAIChat.ts`
  - `components/AIAssistantPanel/hooks/useAIAssistant.ts`

