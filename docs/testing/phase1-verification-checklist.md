# Phase 1 動作確認チェックリスト

## 事前準備

- [ ] Ollamaが起動している（`ollama serve` または自動起動）
- [ ] 開発サーバーが起動できる状態
- [ ] ブラウザの開発者ツールが開ける状態

---

## 1. コンパイル・型チェック

### 確認方法

```bash
# TypeScriptの型チェック（エラーがないことを確認）
npx tsc --noEmit

# または開発サーバーを起動してエラーがないことを確認
npm run dev
```

### 期待される結果

- [ ] コンパイルエラーがない
- [ ] 型エラーがない
- [ ] 開発サーバーが正常に起動する

---

## 2. テストスクリプトの実行

### 確認方法

```bash
# tsxがインストールされている場合
npx tsx scripts/test-local-model-provider.ts

# または ts-node を使用
npx ts-node scripts/test-local-model-provider.ts
```

### 期待される結果

- [ ] すべてのテストが成功する
- [ ] OllamaProviderインスタンスが作成できる
- [ ] ルーティングが正しく動作する
- [ ] ローカルモデル判定が正しく動作する
- [ ] モデル一覧取得が動作する（Ollama起動時）

---

## 3. ブラウザでの動作確認

### 3.1 AIアシスタントパネルを開く

1. アプリケーションを起動
2. AIアシスタントパネルを開く
3. ブラウザの開発者ツール（F12）を開く
4. Consoleタブを確認

### 期待される結果

- [ ] コンソールエラーがない
- [ ] 警告が最小限である

### 3.2 ローカルモデルの選択

1. モデル選択ドロップダウンを開く
2. 「ローカル」タブを選択
3. 利用可能なモデル一覧が表示される

### 期待される結果

- [ ] モデル一覧が正しく表示される
- [ ] モデル名が正しくフォーマットされている
- [ ] エラーメッセージが表示されない

### 3.3 メッセージ送信テスト

1. ローカルモデルを選択（例: `qwen2.5:7b`）
2. 簡単なメッセージを送信（例: 「こんにちは」）
3. 応答が返ってくることを確認

### 期待される結果

- [ ] メッセージが正常に送信される
- [ ] 応答が返ってくる（Ollamaが起動している場合）
- [ ] エラーメッセージが表示されない
- [ ] 応答時間が妥当である

### 3.4 再問い合わせの動作確認

1. Tool呼び出しを含むメッセージを送信
2. 再問い合わせが実行されることを確認

### 期待される結果

- [ ] 再問い合わせが正常に動作する
- [ ] Provider抽象が正しく使用されている
- [ ] エラーが発生しない

---

## 4. コードの動作確認ポイント

### 4.1 useAIChat.tsの変更確認

- [ ] `isLocalModel` → `isLocal` に変更されている
- [ ] `getProviderForModel` が使用されている
- [ ] `getModelConfig` が使用されている
- [ ] 既存の`callOllamaAPI`関数は残っている（互換性のため）

### 4.2 Provider抽象の動作確認

- [ ] `OllamaProvider`が正しくインスタンス化される
- [ ] `chat`メソッドが正しく呼び出される
- [ ] レスポンスが正しく処理される

### 4.3 ルーティングの動作確認

- [ ] `getModelConfig`が正しい設定を返す
- [ ] `getProviderForModel`が正しいプロバイダーを返す
- [ ] プロバイダーキャッシュが機能する

---

## 5. エラーケースの確認

### 5.1 Ollamaが起動していない場合

- [ ] エラーメッセージが適切に表示される
- [ ] アプリケーションがクラッシュしない
- [ ] ユーザーに分かりやすいメッセージが表示される

### 5.2 モデルが存在しない場合

- [ ] エラーメッセージが適切に表示される
- [ ] アプリケーションがクラッシュしない

---

## 6. パフォーマンス確認

### 確認項目

- [ ] 初回のモデル一覧取得時間が妥当
- [ ] メッセージ送信の応答時間が妥当
- [ ] メモリリークがない（長時間使用後も安定）

---

## 7. ログ確認

### 確認すべきログ

ブラウザのコンソールで以下を確認：

- [ ] `[useAIChat]` で始まるログが出力される
- [ ] エラーログが出力されない
- [ ] デバッグ情報が適切に出力される

---

## 問題が発生した場合

### よくある問題と対処法

1. **型エラーが発生する**
   - `npx tsc --noEmit` で詳細を確認
   - 型定義ファイルを確認

2. **OllamaProviderが見つからない**
   - インポートパスを確認
   - ファイルが正しく作成されているか確認

3. **モデル一覧が取得できない**
   - Ollamaが起動しているか確認
   - API URLが正しいか確認

4. **メッセージ送信が失敗する**
   - ネットワークタブでリクエストを確認
   - Ollamaのログを確認

---

## 確認完了後の次のステップ

Phase 1の動作確認が完了したら：

1. [ ] すべてのチェック項目が完了
2. [ ] 問題がないことを確認
3. [ ] Phase 2（llama-server統合）に進む準備が整った

---

## テスト結果記録

### テスト実施日
- 日付: ___________
- 実施者: ___________

### テスト結果
- [ ] すべて成功
- [ ] 一部問題あり（詳細を以下に記録）

### 問題詳細
（問題が発生した場合、ここに詳細を記録）

---

## 参考

- 設計書: `docs/architecture/llama-cpp-integration-design.md`
- テストスクリプト: `scripts/test-local-model-provider.ts`

